## 决策树（Decision Tree）

决策树是可以做分类也可以做回归的，主要学习的是分类决策树。决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

主要优点是模型具有可读性，解释性强，分类速度快。

### 决策树模型

![](imge/DecisionTree_1.png)

如图，由结点和有向边组成，结点有两种类型，内部节点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。

![](imge/DecisionTree_2.png)

**决策树学习**：

本质上是从训练数据集中归纳出一组分类规则。学习算法通常是一个递归得选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。一开始所有训练数据都处于根结点，选择一个最优的特征，按照这一特征将训练集进行划分成子集，使得各个子集在当前条件下最好的分类。如果子集能够被基本正确分类，那就构建叶结点；不能就在子集中继续选取最优特征，继续分割，看不能不能构建叶结点，不能就继续割。。。直到训练集都被分割到叶结点中或者没有合适的特征可以作为划分依据为止。这就生成了一颗决策树。

生成了决策树之后，对现有训练数据集的分类来说能力很好，但对未知数据可能就不太好了，可能会出现过拟合现在，所以需要对决策树进行剪枝，也就是合并叶结点，将其父节点退为新的叶结点，让树变得简单。

也就是说，决策树学习算法包含三个过程：**特征选择**、**决策树的生成**和**决策树的剪枝**。决策树的生成只考虑当前最优的情况，也就是局部最优，而剪枝则是考虑全局最优的情况。

### 特征选择

通常特征选择的准则是**信息增益**或**信息增益比**。

#### 信息增益

书上关于**熵**的定义：在信息论与统计概率中，熵（entropy）是表示随机变量不确定性的度量。设X是一个取**有限个值的离散随机变量**，其概率分布为：

$$
P(X=x_i)=p_i,\ i=1,2,...,n
$$

则随机变量X的熵定义为：

$$
H(X)=-\sum_{i=1}^{n}p_i\log p_i
$$

若$p_i=0$，则定义$0\log0=0$。由定义可知，熵只依赖于X的分布，与X的值无关。熵越大，随机变量的不确定性就越大。从定义可验证$0<= H(p) <= \log n$

条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵（conditional entropy）$H(Y|X)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望

$$
H(Y|X) = \sum_{i=1}^{n}p_iH(Y|X=x_i)
$$

**信息增益（information gain）**：表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。

特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵$H(D|A)$之差，即

$$
g(D,A)=H(D)-H(D|A)
$$

信息增益依赖于特征，不同特征往往具有不同的信息增益。信息增益大的特征具有更强分类能力。**根据信息增益准则的特征选择方法是**：对训练集（或子集）D，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

![](imge/DecisionTree_3.png)

#### 信息增益比

**以信息增益为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题**，使用信息增益比可以对这一问题进行校正。其实就是信息增益多了个分母而已

![](imge/DecisionTree_4.png)

### 决策树的生成

#### ID3算法

核心是在决策树各个结点上应用**信息增益准则选取特征**，递归构建决策树。具体方法是：从根结点开始，对结点计算所有特征的信息增益，选择信息增益最大的作为结点特征，由该特征的不同取值建立子结点；在对子结点递归调用上面的方法，构建决策树。知道信息增益小于一个阈值或者没有特征可以用了。

相当于用极大似然法进行概率模型的选择（不是很能理解这句话）

![](imge/DecisionTree_5.png)
![](imge/DecisionTree_6.png)

#### C4.5的生成算法

C4.5就是把ID3的信息增益换成了信息增益比而已。。。。

![](imge/DecisionTree_7.png)

### 决策树的剪枝

决策树生成算法递归生成，知道某一条件不能满足，这样生成的决策树对训练集来说很好，但是对未知数据就可能没有多准确，也就是过拟合了。所以需要对复杂的树进行简化，也就是剪枝。

大概就是从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化模型。

决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。

一种简单的决策树学习的剪枝算法：

![](imge/DecisionTree_8.png)
![](imge/DecisionTree_9.png)

$C(T)$表示模型对训练集的预测误差，也就是模型和训练集的拟合程度，$|T|$表示模型复杂程度，通过参数$\alpha >= 0$来控制。较大的$\alpha$使得模型简单，相反较小的就使得模型复杂。$\alpha=0$就意味着模型只考虑与训练集的拟合程度，不考虑模型复杂度。

![](imge/DecisionTree_10.png)
![](imge/DecisionTree_11.png)
![](imge/DecisionTree_12.png)

### CART算法（classification and regression tree）

应该是应用最广泛的决策树学习方法，可以用于分类也可以用于回归。同样是由特征选择、树的生成和剪枝组成。

CART算法由两步组成：

1. 决策树生成：基于训练集生成决策树，生成的决策树要尽量大
2. 决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准

#### CART生成

决策树的生成就是递归地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，选择特征，生成二叉树

1. 回归树的生成

    设将输入空间划分为M个单元$R_1,R_2,...,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树的模型可表示为：

    $$
    f(x)=\sum_{m=1}^{M}c_mI(x\in R_m)
    $$

    确定空间划分之后，就可以用平方误差来表示预测结果与训练集的误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$。
    
    然后是需要知道如何对输入空间进行划分。

    ![](image/../imge/DecisionTree_13.png)
    ![](image/../imge/DecisionTree_14.png)

    ![](image/../imge/DecisionTree_15.png)
2. 分类树生成

    用基尼指数选择最优特征，同时决定该特征的最优二值切分点

    $$
    Gini(p) = \sum_{k=1}^{K}p_k(1-p_k) = 1 - \sum_{k=1}^{K}p_k^2
    $$

    基尼指数大概也是一个类似于概率分布的东西，主要知道：基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D,A)表示经过A=a分割后集合D的不确定性。基尼指数数值越大，样本集合的不确定性也就越大，有点像熵。

    ![](imge/DecisionTree_15.png)

#### CART剪枝

CART剪枝算法由两步组成：

1. 从生成算法产生的决策树$T_0$地段开始不断剪枝，知道$T_0$的根结点，形成一个子树序列$\{T_0,T_1,...,T_n\}$
2. 通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树

![](image/../imge/DecisionTree_17.png)


### 参考资料

- [带读 李航 《统计学习方法 第二版》 第五章 决策树 跟我一起学习成长为大神_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1RC4y1p7wX?p=9&spm_id_from=pageDriver)
- 《统计学习方法（第二版）》