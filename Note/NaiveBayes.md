## 朴素贝叶斯

朴素贝叶斯是一种生成模型。

**生成模型和判别模型的区别**：从建模方法上理解，生成模型分类是对每一种类别都进行建模的，然后对输入的数据匹配每一个类别的建模，取概率最大的一个作为输出类别；而判别模型则是找到类别之间的区别进行建模，输入的数据直接通过判别的式子即可得到类别。

简单的例子就是：判断一个人说话是说日语，英语还是中文，生成模型的方法是直接学习三种语言，然后听到这个人说话就是知道是哪一种语言；而判别模型的方法是找到三者之间的区别，求取三者的一些特征区别做一个划分，然后再对这个人说的话进行判断。好像不太好的例子。。。。。

### 模型定义

朴素贝叶斯的基本思想就是算出一个输入数据属于每一种类别的概率，然后取最大的那一个，$P(X=x|Y=C_k)$

通过贝叶斯公式可以得到

$$
\begin{aligned}
P(Y=C_k|X=x) &=  \frac{P(X=x|Y=C_k)P(Y=C_k)}{P(X=x)} \\
&=\frac{P(X=x|Y=C_k)P(Y=C_k)}{\sum_k P(X=x,Y=C_k)} \\
&=\frac{P(X=x|Y=C_k)P(Y=C_k}{\sum_k P(X=x|Y=C_k)P(Y=C_k)} \\
&=\frac{P(Y=C_k)\prod_j(X^{(j)}=x^{(j)}|Y=C_k)}{\sum_k{P(Y=C_k)\prod_j P(X^{(j)}=x^{(j)}|Y=C_k)}}
\end{aligned}
$$

上面式子中其实有一个**强假设**，也是朴素贝叶斯为何朴素的原因：实际上应该是$P(X=x|Y=C_k)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=C_k)$，因为特征之间可能是的联系的，所以按理来说应该是需要考虑特征之间联系的情况。但是如果考虑特征之间的联系，参数就会变得十分复杂，参数取值是指数级增长的，会变得非常难以计算，所以朴素贝叶斯的强假设就是**条件独立性**，也就是假设每个特征之间是相互独立的。当然，这样是会损失一定的分类准确率的。

推导过程其实很好理解，就不废话了。。。。。。。。。

最后得到的可以发现对于每一个$C_k$，分母都是一样的，所以优化时可以当作常数。直接得到最终的模型：

$$
\begin{aligned}
f(x)=argmax_{C_k} P(Y=C_k|X=x) &=P(Y=C_k)\prod_j P(X^{(j)}=x^{(j)}|Y=C_k)
\end{aligned}
$$

### 参数估计

简单来说就是根据上面得到的，计算出$P(Y=C_k)$和$P(X^{(j)}=x^{(j)}|Y=C_k)$

$P(Y=C_k)$就是计算出每一个$Y$的类别的个数的占比。

$P(X^{(j)}=x^{(j)}|Y=C_k)$的计算比较麻烦，就是计算当$Y$是每一类别时，对于某一个特征来说各个特征值的占比。

实际上看了《统计学习方法（第二版）》的列题4.1之后很好理解。

**贝叶斯估计**：

也就是加了个拉普拉斯平滑。简单来说，就是上面的式子中可能出现分母等于0的情况，或者是概率值等于0做连乘的适合导致整一个结果为0.为了防止这个情况分子分母分别加入一个项

于是变成了：

$$
\begin{aligned}
P(X^{(j)}=a_{jl}|Y=C_k) &= \frac{\sum_{i=1}^N I(x_i^{(j)}=a_{jl},y_i=C_k) + \lambda}{\sum_{i=1}^N I(y_i=C_k) + S_j\lambda }
\end{aligned}
$$

$S_j$是特征值的种类，（不知道怎么说，就是那一列的特征值去重吧。。。。。）


**高斯朴素贝叶斯**：

上面的朴素贝叶斯，只能处理离散特征，想要处理当然可以对连续特征进行离散化，但是粒度之类的控制比较难搞，也可以使用高斯朴素贝叶斯。

当特征属性为连续值时，通常假定其值服从高斯分布（也称正态分布），高斯分布有两个参数，均值$\mu$和方差$\sigma^2$，对于每个类别$y = C_k$：
$$
P(X = x|Y=y)=\frac{1}{(2\pi)^{\frac{1}{2}}\sigma^2}\exp(-\frac{(x-\mu)^2}{2\sigma^2})
$$


### 代码

实验课有需要写，就大概弄了一下，然后也做了一下《统计学习方法（第二版）》的例题4.1

- [朴素贝叶斯实验代码（和逻辑回归写到一起了）](../Code/LogisticRegressionAndNaiveBayes.ipynb)
- [《统计学习方法（第二版）》例题4.1](../Code/NaiveBayes_4_1.py)
