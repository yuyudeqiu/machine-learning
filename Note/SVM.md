## 支持向量机（support vector machines，SVM）

支持向量机学习方法包含构建有简至繁的模型：**线性可分支持向量机**、**线性支持向量机**以及**非线性支持向量机**。

当数据线性可分时，通过硬间隔最大化，学习一个线性的分类器，即线性可分支持向量机；当数据近似线性可分时，通过软间隔最大化，也可以学习一个线性分类器，即线性支持向量机；当数据线性不可分时，通过核技巧及软间隔最大化，学习非线性支持向量机。

### 线性可分支持向量机

学习目标是在特征空间中找到一个分离超平面，将实例分到不同的类。分离超平面对应方程$wx+b = 0$，由法向量和截距决定，基本和感知机一样。区别于感知机的是，感知机利用误分类最小化的策略，求的分离超平面可以有无穷多个解，**线性可分支持向量机利用间隔最大化求解最优超平面，这样的解是唯一的**。

其实和感知机真的有些像，先学过了感知机再看支持向量机就会有点感觉了。

给定数据集（线性可分），通过间隔最大化得到分离超平面，模型，其实原来还是一个线性函数，然后再套上一个类似激活函数，跟感知机一样的sign：

$$
w^* x + b^* = 0 \\
f(x) = sign(w^* x + b^*)
$$

这就是线性可分的支持向量机

然后就是看如何最大化这个间隔，然后把他转换成最小化问题，以及搞清楚函数间隔和几何间隔。然后SVM使用的几何间隔。

**函数间隔和几何间隔**：

简单来说，间隔，就是点到超平面的距离

函数间隔是这样的$\hat{\gamma_i} = y_i(wx_i+b)$，没有任何约束，然后优化目标就是要找到所有点中，函数间隔最小的一个或几个然后把他们最大化。但是函数间隔有一个问题，比如成比例的同时改变参数为$2w$和$2b$，超平面不会发生改变，但是函数间隔却变了成原来的两倍。

集合间隔就是对函数间隔加以约束，如规范化，$||w|| = 1$，使得间隔是确定的。

**间隔最大化：**

间隔最大化的直观解释：找到间隔最大化的超平面意味着被分隔的训练集分类可信度是最高的，也就是说两类数据来说距离越远，这样的超平面就对新的实例有很好的预测能力。

最大间隔分离超平面可以这样表示成一个**约束最优化问题**：

$$
\max_{w,b} \gamma \\
s.t. \ y_i(\frac{w}{||w||}x_i + \frac{b}{||w||})  \gamma ,\  i=1,2,...,N
$$

也就是希望最大化超平面$(w,b)$关于训练集的几何间隔$\gamma$，约束条件表示超平面关于每个训练样本点的集合间隔至少是$\gamma$

然后还需要考虑到函数间隔和集合间隔的问题

$$
\max_{w,b} \frac{\hat{\gamma}}{||w||} \\
s.t. \ y_i(\frac{w}{||w||}x_i + \frac{b}{||w||})  \hat{\gamma} ,\  i=1,2,...,N
$$

实际上使用函数间隔也不影响最优化问题的解。即使是等比例改变$w$和$b$，函数间隔也会等比例，不过这一改变对优化问题等式约束没有影响。这样的话，如果把$\hat{\gamma}$取为1即$\hat{\gamma} = 1$，代入带上面最优化问题可以得到最大化的是$\frac{1}{||w||}$，然后可以转换成最小化问题$\frac{1}{2}||w||^2$。就得到了线性可分支持向量机学习的最优化问题：

$$
\min_{w,b} \frac{1}{2}||w||^2 \\
s.t. y_i(wx_i+b)-1 >= 0 , \ i=1,2,...,N
$$

变成凸优化问题（虽然不懂凸优化是什么。。。。）

得到最优化问题的解$w^*$,$b^*$就可以得到最大化间隔分离超平面$w^*x+b^*=0$，套上sign函数就是分类决策函数$f(x) = sign(w^*x+b^*)$，也就是线性可分支持向量机模型。

**学习算法**：

![](image/../imge/SVM_1.png)
![](image/../imge/SVM_2.png)


**支持向量**：

支持向量实际上就是落在**间隔边界**上的样本点，关于间隔边界的就是约束最优化问题种的约束条件，等号成立的时候，即$y_i(wx_i+b)-1 = 0$。支持向量就是落在两条线上，这两天线之间就是间隔边界。

![](imge/SVM_4.png)

实际上再决定分类超平面时，只有支持向量起作用，离超平面远的那些点没有作用，甚至可能把这些点直接删掉也不影响结果。

**学习的对偶算法**：

跟感知机一样，SVM也有学习的对偶算法，使得更容易求解。然后SVM使用对偶形式的另一个原因是可以引入和函数，然后推广到非线性分类

由上述式子得到拉格朗日函数（没学仔细学过但是大概知道有这么一个公式能用。。。）：

$$
L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum_{i=1}^{N}\alpha_iy_i(wx_i+b)+\sum_{i=1}^{N}\alpha_i
$$

其中的$\alpha = (\alpha_1,\alpha_2,...,\alpha_N)^T$是拉格朗日乘子向量。

原始问题的对偶问题是极大极小问题：

$$
\max_\alpha \min_{w,b} L(w,b,\alpha)
$$

要先对$L(w,b,\alpha)$求$w,b$的极小，然后再对$\alpha$求极大。

求偏导 = 0：

$$
\begin{aligned}
  &\nabla_wL(w,b,\alpha) = w - \sum_{i=1}^{N}\alpha_iy_ix_i = 0 \\
  &\nabla_bL(w,b,\alpha) = - \sum_{i=1}^{N}\alpha_iy_i=0
\end{aligned}
$$

得到：

$$
\begin{aligned}
  & w = \sum_{i=1}^{N}\alpha_iy_ix_i \\ 
  & \sum_{i=1}^{N}\alpha_iy_ix_i
\end{aligned}
$$

带入到拉格朗日函数，最后简化得到：

$$
\min_{w,b}L(w,b,\alpha) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i\alpha_jy_iy_j(x_ix_j) + \sum_{i=1}^{N}\alpha_i
$$

求$\min_{w,b}(w,b,\alpha)$ 对$\alpha$的极大，即是对偶问题

$$
\begin{aligned}
  & \max_\alpha - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i x_j) + \sum_{i=1}^{N} \alpha_i \\
  & s.t. \  \sum_{i=1}^{N} \alpha y_i = 0 \\
  & \alpha_i >= 0, \ i=1,2,...,N
\end{aligned}
$$

转换成求极小的等价问题：

$$
\begin{aligned}
  & \min_\alpha  \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_i \alpha_j y_i y_j (x_i x_j) - \sum_{i=1}^{N} \alpha_i \\
  & s.t. \  \sum_{i=1}^{N} \alpha y_i = 0 \\
  & \alpha_i >= 0, \ i=1,2,...,N
\end{aligned}
$$

通过定理C.3 看KT条件成立，可以得到原始最优化问题的解$w^*,b^*$：

$$
\begin{aligned}
  & w^* = \sum_{i=1}^{N} \alpha_i^*y_ix_i \\
  & b^* = y_i - \sum_{i=1}^{N}\alpha_i^*y_i(x_ix_j)
\end{aligned}
$$

分类决策最后写出来就是：

$$
f(x) = sign(\sum_{i=1}^{N}\alpha_i^*y_i(xx_i)+b^*)
$$

![](./image/../imge/SVM_3.png)

可以看到线性可分支持向量机种，$w^*和b^*$只依赖于训练集种对应于$\alpha_i^*>0$ 的样本点 $(x_i,x_j)$，而其他样本点对 $w^*$ 和 $b^*$没有影响。$\alpha_i^*>0$的实例点$x_i \in R^n$ 称之为支持向量。

### 线性支持向量机（软间隔最大化）

很多时候，数据都是线性不可分的，存在一些噪声或特异点，出去这些点后大部分样本还是基本线性可分的。对于这样的数据集，就可以使用软间隔最大化来学习。

对硬间隔最大化的约束条件再引进一个松弛变量