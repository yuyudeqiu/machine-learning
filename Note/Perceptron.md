## 感知机（Perceptron Algorithm）

感知机的思想是**错误驱动**。

假定一个数据集是**线性可分**的，如图：

![](imge/LinearClassification_2.png)

目标就是要找到一个超平面，可以将两种不同类类别完全分离开。


基本步骤大概是：初始化一个$w$，一开始是会有错误分类的，然后根据被错误分类的样本集$D$，通过迭代一步步优化$w$，直到找到一个正确分类的超平面。

### 模型定义

$$
f(x)=sign(w^Tx) \\
其中：sign(a)=\begin{cases}
  +1,\ a>0 \\
  -1,\ a<0 
\end{cases}
$$

### 优化策略

也就是一个loss function

$$
  L(w)=\sum_{i=1}^{N}I(y_iw^Tx<0)
$$

$I(y_iw^Tx<0)$可以很容易解释，根据$sign(a)$可以知道，当样本正确分类的时候，$w^Tx_i>0, y_i=+1$；$w^Tx_i<0,y_i=-1$。这时两个式子可以合并为一个$y_iw^Tx_i>0$。相反的如果错误分类，则是$y_iw^Tx<0$，所以上面的$I()$是统计错误分类的个数。也就是最小化分类错误的个数。

但是这个指示函数$I()$的值只有是0，1，存在跳跃间断点，不是连续可导的，所以很难求解，需要从另一个角度去求解。所以应该从$y_iw^Tx<0$去看。因为它本身的连续可导的。

所以直接把$y_iw^Tx<0$作为loss function

$$
L(w)=\sum_{x_i\in D} -y_iw^Tx_i
$$

实际上如果加上一个$L_2$范数，$\frac{1}{||w||} \sum_{x_i\in D} -y_iw^Tx_i$，这个就是空间中点到超平面的距离总和。但是感知机采用的是不考虑$\frac{1}{||w||}$，得到的作为loss function

然后就是对这个东西求偏导了。。很简单的得到梯度，再利用随机梯度下降法：

$$
\nabla _w L=-y_ix_i \\
w^{t+1} \leftarrow w^{t} -\lambda \nabla_wL \\
即w^{t+1} \leftarrow w^{t} +\lambda  y_i x_i \\
$$

### 感知机学习算法的对偶形式

从上面的式子可以发现，w和b的变化，都只与错误分类的点有关。而每一次都是变化都是一个定值（可以说是定值）也就是$\lambda y_ix_i$

前面用的表示方法一直没有用到$b$，是因为把$b$也并入了$w$之中，然后是设定$x^{(0)} = 1$，这里为了方便理解还是把$b$还原出来了。

也就是之前的一个更新规则变成了：

$$
\begin{aligned}
  &w \leftarrow w + \lambda y_ix_i \\
  &b \leftarrow b + \lambda y_i
\end{aligned}
$$

通过逐步修改$w$和$b$，设修改n次，则$w$和$b$关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i$和$\alpha_iy_i$，这里$\alpha_i=n_i\lambda$，这样$w$和$b$可以表示成：

$$
\begin{aligned}
  w &= \sum_{i=1}^{N} \alpha_iy_ix_i \\
  b &= \sum_{i=1}^{N} \alpha_iy_i
\end{aligned}
$$

这里的$\alpha$可以理解为是每个点的更新次数，也就是错误分类的次数，假如有N个样本，那$\alpha$就可以看成是一个N个大小的数组，初始为0。对$\alpha$里的某一个值来说，如果数越大，则离超平面越近，越难被正确分类，也就是对学习结果影响最大

大概的换成对偶形式，除了表达式上的一点小变化，还有就是更新规则的小小变化：

模型：

$$
f(x) = sign(\sum_{j=1}^{N}\alpha_jy_jx_jx+b)
$$

更新规则：

$$
\begin{aligned}
if \ y_i(\sum_{j=1}^{N}\alpha_Jy_jx_jx_i+b) <= 0 \\
  \alpha_i &\leftarrow \alpha_i + \lambda \\
  b &\leftarrow b + \lambda y_i
\end{aligned}
$$


### 代码复现

这是感知机算法的初始形式。下面是非常粗糙的代码复现：

- [粗糙实现了《统计学习方法（第二版）》的例题2.1](../Code/perceptron.py)
- [粗糙实现了《统计学习方法（第二版）》的例题2.2](../Code/perceptron_2.py)

由于python没有系统学过，很多东西都是现查现用，加上剩下一个月期末，没有上过课只能自己学着来也没有很多时间。。。就先潦草一下了。没有注释。。。是因为，太简单了，就没必要什么很难的注释了吧。这两个算是没有参考过任何其他人的代码，也算写的有点成就感吧。。。还是差很多，菜鸡进步之路。

### Ending

感知机算是一个非常简单的模型了，但是性能很差，无法得到一个特别好的超平面，只要是所有分类都正确了，就停下，并不能像支持向量机一样找到一个最优化的超平面。拿来做编程入手也还算可以。

**参考资料**：


- [【机器学习】带读李航 第一章-第四章《统计学习方法 第二版 监督学习》 跟我一起从菜鸟成长为大神_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1W7411N7Ag?p=44&spm_id_from=pageDriver)
- 《统计学习方法（第二版）- 李航》
