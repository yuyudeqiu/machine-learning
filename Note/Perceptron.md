## 感知机（Perceptron Algorithm）

感知机的思想是**错误驱动**。

假定一个数据集是**线性可分**的，如图：

![](imge/LinearClassification_2.png)

目标就是要找到一个超平面，可以将两种不同类类别完全分离开。


基本步骤大概是：初始化一个$w$，一开始是会有错误分类的，然后根据被错误分类的样本集$D$，通过迭代一步步优化$w$，直到找到一个正确分类的超平面。

### 模型定义

$$
f(x)=sign(w^Tx) \\
其中：sign(a)=\begin{cases}
  +1,\ a>0 \\
  -1,\ a<0 
\end{cases}
$$

### 优化策略

也就是一个loss function

$$
  L(w)=\sum_{i=1}^{N}I(y_iw^Tx<0)
$$

$I(y_iw^Tx<0)$可以很容易解释，根据$sign(a)$可以知道，当样本正确分类的时候，$w^Tx_i>0, y_i=+1$；$w^Tx_i<0,y_i=-1$。这时两个式子可以合并为一个$y_iw^Tx_i>0$。相反的如果错误分类，则是$y_iw^Tx<0$，所以上面的$I()$是统计错误分类的个数。也就是最小化分类错误的个数。

但是这个指示函数$I()$的值只有是0，1，存在跳跃间断点，不是连续可导的，所以很难求解，需要从另一个角度去求解。所以应该从$y_iw^Tx<0$去看。因为它本身的连续可导的。

所以直接把$y_iw^Tx<0$作为loss function

$$
L(w)=\sum_{x_i\in D} -y_iw^Tx_i
$$

实际上如果加上一个$L_2$范数，$\frac{1}{||w||} \sum_{x_i\in D} -y_iw^Tx_i$，这个就是空间中点到超平面的距离总和。但是感知机采用的是不考虑$\frac{1}{||w||}$，得到的作为loss function

然后就是对这个东西求偏导了。。很简单的得到梯度，再利用随机梯度下降法：

$$
\nabla _w L=-y_ix_i \\
w^{t+1} \leftarrow w^{t} -\lambda \nabla_wL \\
即w^{t+1} \leftarrow w^{t} +\lambda  y_i x_i \\
$$

这是感知机算法的初始形式。下面的一个非常粗糙的代码复现

[粗糙实现了《统计学习方法（第二版）》的例题2.4](../Code/perceptron.py)

